{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7432cff",
   "metadata": {},
   "source": [
    "\n",
    "# Stage 04 — Data Acquisition & Ingestion\n",
    "\n",
    "This notebook pulls **one market-related dataset via API** (FRED) and **scrapes one public table** (Wikipedia), validates each, and saves timestamped CSVs to `data/raw/`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdcdd76",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup\n",
    "**Repo structure expected:**\n",
    "```\n",
    "data/raw/\n",
    "notebooks/stage04_data-acquisition-and-ingestion.ipynb\n",
    ".env           # (not committed)\n",
    ".env.example\n",
    ".gitignore\n",
    "```\n",
    "Ensure `.env` contains `FRED_API_KEY=...`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e92e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "from typing import Iterable\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().resolve().parent if (Path.cwd().name == \"notebooks\") else Path.cwd()\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_RAW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load secrets\n",
    "load_dotenv()\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Raw data dir:\", DATA_RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b4dcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ts_stamp() -> str:\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "def save_csv(df: pd.DataFrame, prefix: str, *parts: str) -> Path:\n",
    "    name = \"_\".join([prefix, *[p.replace(\" \", \"-\") for p in parts], ts_stamp()]) + \".csv\"\n",
    "    out = DATA_RAW / name\n",
    "    df.to_csv(out, index=False)\n",
    "    print(\"Saved:\", out)\n",
    "    return out\n",
    "\n",
    "def expect_columns(df: pd.DataFrame, required: Iterable[str]) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    assert not missing, f\"Missing required columns: {missing}\"\n",
    "\n",
    "def assert_no_all_na(df: pd.DataFrame, cols: Iterable[str]) -> None:\n",
    "    bad = [c for c in cols if df[c].isna().all()]\n",
    "    assert not bad, f\"Columns entirely NA: {bad}\"\n",
    "\n",
    "def type_parse(df: pd.DataFrame, date_cols: Iterable[str]=(), float_cols: Iterable[str]=()) -> pd.DataFrame:\n",
    "    for c in date_cols:\n",
    "        df[c] = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
    "    for c in float_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def quick_validation_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    return pd.DataFrame({\n",
    "        \"dtype\": df.dtypes.astype(str),\n",
    "        \"non_null\": df.notna().sum(),\n",
    "        \"nulls\": df.isna().sum(),\n",
    "        \"null_pct\": (df.isna().mean()*100).round(2),\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9538e76f",
   "metadata": {},
   "source": [
    "\n",
    "## 1) API Pull — FRED (10Y Treasury `DGS10`)\n",
    "**Endpoint:** `https://api.stlouisfed.org/fred/series/observations`  \n",
    "**Auth:** `FRED_API_KEY` in `.env`  \n",
    "**Params:** `series_id=DGS10`, `observation_start=2010-01-01`, `file_type=json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac6aa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FRED_KEY = os.getenv(\"FRED_API_KEY\")\n",
    "assert FRED_KEY, \"Put FRED_API_KEY in your .env (and never commit .env).\"\n",
    "\n",
    "def fred_series(series_id: str, start: str=\"2010-01-01\", end: str|None=None) -> pd.DataFrame:\n",
    "    url = \"https://api.stlouisfed.org/fred/series/observations\"\n",
    "    params = {\n",
    "        \"series_id\": series_id,\n",
    "        \"api_key\": FRED_KEY,\n",
    "        \"file_type\": \"json\",\n",
    "        \"observation_start\": start,\n",
    "    }\n",
    "    if end:\n",
    "        params[\"observation_end\"] = end\n",
    "    r = requests.get(url, params=params, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    j = r.json()\n",
    "    obs = pd.DataFrame(j.get(\"observations\", []))\n",
    "    if obs.empty:\n",
    "        raise ValueError(f\"No observations returned for {series_id}\")\n",
    "    obs = obs.rename(columns={\"date\":\"date\", \"value\":\"value\"})[[\"date\",\"value\"]]\n",
    "    obs = type_parse(obs, date_cols=[\"date\"], float_cols=[\"value\"])\n",
    "\n",
    "    expect_columns(obs, [\"date\",\"value\"])\n",
    "    assert_no_all_na(obs, [\"value\"])\n",
    "    assert len(obs) > 100, f\"Unexpectedly few rows: {len(obs)}\"\n",
    "    return obs\n",
    "\n",
    "api_df = fred_series(\"DGS10\", start=\"2010-01-01\")\n",
    "display(api_df.head())\n",
    "display(quick_validation_report(api_df))\n",
    "api_path = save_csv(api_df, \"api\", \"FRED\", \"DGS10\")\n",
    "api_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c67fb3",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Scrape — Wikipedia S&P 500 Table\n",
    "**URL:** https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\n",
    "We grab the first `wikitable` and validate presence of key columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c502d818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "WIKI_URL = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "def scrape_sp500_table(url: str=WIKI_URL) -> pd.DataFrame:\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "    assert table, \"Could not find target table\"\n",
    "    headers = [th.get_text(strip=True) for th in table.find_all(\"tr\")[0].find_all(\"th\")]\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        tds = [td.get_text(strip=True) for td in tr.find_all([\"td\",\"th\"])]\n",
    "        if len(tds) == len(headers):\n",
    "            rows.append(tds)\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    expected_subset = {\"Symbol\",\"Security\",\"GICS Sector\"}\n",
    "    if expected_subset.isdisjoint(set(df.columns)):\n",
    "        raise AssertionError(f\"Expected any of {sorted(expected_subset)} in columns, got: {list(df.columns)[:6]}...\")\n",
    "    for c in [\"Symbol\",\"Security\"]:\n",
    "        if c in df.columns:\n",
    "            assert_no_all_na(df, [c])\n",
    "    return df\n",
    "\n",
    "scrape_df = scrape_sp500_table()\n",
    "display(scrape_df.head())\n",
    "display(quick_validation_report(scrape_df).head(8))\n",
    "scrape_path = save_csv(scrape_df, \"scrape\", \"wikipedia\", \"sp500_table\")\n",
    "scrape_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725ac149",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Documentation — Sources, Params, Validation, Assumptions\n",
    "**API:** FRED observations endpoint for `DGS10` (10Y Treasury); params: `series_id`, `observation_start`, `file_type=json`; auth via `.env`.\n",
    "**Scrape:** Wikipedia “List of S&P 500 companies” — first `wikitable`.\n",
    "**Validation:** Column presence, dtype parsing (date/float), NA audit, and minimum row count for API.\n",
    "**Assumptions/Risks:** Wikipedia structure may change; FRED returns missing values as '.' which are coerced to NaN.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
